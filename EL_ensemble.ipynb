{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "**Required files in DATA_DIR:**\n",
    "- `metrics.xlsx`: Excel file with model predictions (columns: `model_name`, `predicts`, `tag`)\n",
    "- `test_labels.txt`: Text file with true labels (one label per line, 0 or 1)\n",
    "\n",
    "**Expected format:**\n",
    "- Each row in Excel = one model run\n",
    "- `predicts` column = comma-separated probabilities for test samples\n",
    "- `tag` column = run identifier (same tag = same run across different models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import libraries and load data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import os\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    confusion_matrix, roc_auc_score, classification_report\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set data paths\n",
    "DATA_DIR = '../data'\n",
    "excel_path = os.path.join(DATA_DIR, 'metrics.xlsx')\n",
    "labels_path = os.path.join(DATA_DIR, 'test_labels.txt')\n",
    "\n",
    "# Load Excel file with model predictions\n",
    "df_models = pd.read_excel(excel_path)\n",
    "\n",
    "# Load true labels from test set\n",
    "with open(labels_path, 'r') as file:\n",
    "    true_labels = np.array([int(line.strip()) for line in file])\n",
    "\n",
    "print(f\"Loaded {len(df_models)} model results\")\n",
    "print(f\"Loaded {len(true_labels)} true labels (test set)\")\n",
    "print(f\"Label distribution: {np.sum(true_labels)} positive class, {len(true_labels) - np.sum(true_labels)} negative class\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Check loaded data\n",
    "# Display first few rows to verify data loading\n",
    "print(\"First 5 rows of model data:\")\n",
    "print(df_models.head())\n",
    "\n",
    "# Show columns\n",
    "print(\"\\nColumns in the dataset:\")\n",
    "print(df_models.columns.tolist())\n",
    "\n",
    "# Show unique models\n",
    "print(\"\\nUnique models:\")\n",
    "print(df_models['model_name'].unique())\n",
    "\n",
    "# Check number of runs per model - let's find the column that identifies runs\n",
    "if 'tag' in df_models.columns:\n",
    "    runs_per_model = df_models.groupby('model_name')['tag'].nunique()\n",
    "    print(\"\\nNumber of unique runs per model:\")\n",
    "    print(runs_per_model)\n",
    "    \n",
    "    # Check unique run identifiers\n",
    "    print(\"\\nUnique run identifiers (tags):\")\n",
    "    print(df_models['tag'].unique()[:10])  # Show first 10\n",
    "else:\n",
    "    print(\"\\nNo 'tag' column found. Available columns:\")\n",
    "    print(df_models.columns.tolist())\n",
    "    \n",
    "    # Try to identify which column might be the run identifier\n",
    "    for col in df_models.columns:\n",
    "        if df_models[col].dtype in ['int64', 'int32', 'float64']:\n",
    "            unique_vals = df_models[col].nunique()\n",
    "            if unique_vals <= 100:  # Potential run identifier\n",
    "                print(f\"Column '{col}' has {unique_vals} unique values - could be run identifier\")\n",
    "                print(f\"First 10 values: {df_models[col].unique()[:10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Data Structure\n",
    "Check that data loaded correctly and inspect required columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define ensemble functions\n",
    "def parse_predictions(pred_string):\n",
    "    \"\"\"Parse the prediction string into a numpy array\"\"\"\n",
    "    try:\n",
    "        # Handle string representation of predictions\n",
    "        if isinstance(pred_string, str):\n",
    "            # Handle different possible formats\n",
    "            # Remove any brackets, parentheses, and split by comma or space\n",
    "            cleaned = pred_string.strip('[](){} ')\n",
    "            # Split by comma first, if not present, split by space\n",
    "            if ',' in cleaned:\n",
    "                items = cleaned.split(',')\n",
    "            else:\n",
    "                items = cleaned.split()\n",
    "            \n",
    "            # Convert to float array\n",
    "            return np.array([float(x.strip()) for x in items if x.strip()])\n",
    "        elif isinstance(pred_string, (list, np.ndarray)):\n",
    "            return np.array(pred_string)\n",
    "        else:\n",
    "            print(f\"Unexpected type for predictions: {type(pred_string)}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing: {pred_string}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def hard_voting_ensemble(predictions_list):\n",
    "    \"\"\"Perform hard voting ensemble\"\"\"\n",
    "    # Convert probabilities to binary predictions\n",
    "    binary_preds = [np.round(preds) for preds in predictions_list]\n",
    "    # Stack and calculate majority vote\n",
    "    stacked = np.vstack(binary_preds)\n",
    "    return np.round(np.mean(stacked, axis=0))\n",
    "\n",
    "def soft_voting_ensemble(predictions_list):\n",
    "    \"\"\"Perform soft voting ensemble\"\"\"\n",
    "    # Average probabilities\n",
    "    avg_probs = np.mean(predictions_list, axis=0)\n",
    "    # Convert to binary predictions\n",
    "    return np.round(avg_probs)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_prob=None):\n",
    "    \"\"\"Calculate all classification metrics\"\"\"\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred),\n",
    "        'recall': recall_score(y_true, y_pred),\n",
    "        'sensitivity': recall_score(y_true, y_pred),  # Same as recall\n",
    "        'f1_score': f1_score(y_true, y_pred),\n",
    "        'confusion_matrix': confusion_matrix(y_true, y_pred),\n",
    "    }\n",
    "    \n",
    "    # Calculate specificity from confusion matrix\n",
    "    tn, fp, fn, tp = metrics['confusion_matrix'].ravel()\n",
    "    metrics['specificity'] = tn / (tn + fp)\n",
    "    \n",
    "    # Calculate AUC if probability predictions are available\n",
    "    if y_prob is not None:\n",
    "        metrics['auc'] = roc_auc_score(y_true, y_prob)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def create_ensemble_by_run(model_names, run_number, voting_method='soft'):\n",
    "    \"\"\"Create an ensemble from selected models using the same run number (based on order)\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        # Get all runs for this model\n",
    "        model_data = df_models[df_models['model_name'] == model_name].reset_index(drop=True)\n",
    "        \n",
    "        # Check if run_number is within bounds\n",
    "        if run_number > len(model_data):\n",
    "            print(f\"Warning: Run {run_number} not available for {model_name} (only {len(model_data)} runs)\")\n",
    "            continue\n",
    "        \n",
    "        # Get the specific run (0-indexed, so subtract 1)\n",
    "        specific_run = model_data.iloc[run_number - 1]\n",
    "        \n",
    "        # Parse predictions\n",
    "        preds = parse_predictions(specific_run['predicts'])\n",
    "        if preds is not None:\n",
    "            # Ensure we only use the first 46 predictions (test set size)\n",
    "            preds = preds[:46]\n",
    "            predictions.append(preds)\n",
    "    \n",
    "    if not predictions:\n",
    "        raise ValueError(f\"No valid predictions found for run {run_number}\")\n",
    "    \n",
    "    # Perform ensemble voting\n",
    "    if voting_method == 'hard':\n",
    "        ensemble_pred = hard_voting_ensemble(predictions)\n",
    "        ensemble_prob = None\n",
    "    else:  # soft voting\n",
    "        ensemble_prob = np.mean(predictions, axis=0)\n",
    "        ensemble_pred = np.round(ensemble_prob)\n",
    "    \n",
    "    return ensemble_pred, ensemble_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Create multiple ensembles (one per run)\n",
    "\n",
    "\n",
    "\n",
    "# Configuration: Choose models for the ensemble\n",
    "# Replace these with model names from the 'model_name' column in the Excel file\n",
    "models_to_use = ['InceptionResNetV2', 'DenseNet201', 'Xception']\n",
    "\n",
    "# Voting method: 'hard' or 'soft'\n",
    "voting_method = 'hard'\n",
    "\n",
    "# Create ensembles for all runs\n",
    "num_runs = 100  # Adjust if needed\n",
    "ensemble_results = []\n",
    "\n",
    "for run_num in range(1, num_runs + 1):\n",
    "    try:\n",
    "        ensemble_pred, ensemble_prob = create_ensemble_by_run(models_to_use, run_num, voting_method)\n",
    "        metrics = calculate_metrics(true_labels, ensemble_pred, ensemble_prob)\n",
    "        \n",
    "        result = {\n",
    "            'run_number': run_num,\n",
    "            'predictions': ensemble_pred,\n",
    "            'probabilities': ensemble_prob,\n",
    "            'metrics': metrics\n",
    "        }\n",
    "        ensemble_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating ensemble for run {run_num}: {str(e)}\")\n",
    "\n",
    "# Display median metrics across all runs\n",
    "if ensemble_results:\n",
    "    median_metrics = {\n",
    "        'accuracy': np.median([r['metrics']['accuracy'] for r in ensemble_results]),\n",
    "        'precision': np.median([r['metrics']['precision'] for r in ensemble_results]),\n",
    "        'recall': np.median([r['metrics']['recall'] for r in ensemble_results]),\n",
    "        'specificity': np.median([r['metrics']['specificity'] for r in ensemble_results]),\n",
    "        'f1_score': np.median([r['metrics']['f1_score'] for r in ensemble_results]),\n",
    "        'auc': np.median([r['metrics']['auc'] for r in ensemble_results if 'auc' in r['metrics']])\n",
    "    }\n",
    "    \n",
    "    ensemble_name = f\"{voting_method}_\" + \"_\".join(models_to_use)\n",
    "    print(f\"\\nEnsemble: {ensemble_name}\")\n",
    "    print(f\"Median metrics across {len(ensemble_results)} runs:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Accuracy: {median_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {median_metrics['precision']:.4f}\")\n",
    "    print(f\"Recall/Sensitivity: {median_metrics['recall']:.4f}\")\n",
    "    print(f\"Specificity: {median_metrics['specificity']:.4f}\")\n",
    "    print(f\"F1-Score: {median_metrics['f1_score']:.4f}\")\n",
    "    print(f\"AUC: {median_metrics['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Ensemble Functions\n",
    "Functions for hard voting (majority vote) and soft voting (average probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4.5: Confusion Matrix Analysis\n",
    "def analyze_confusion_matrices(ensemble_results):\n",
    "    \"\"\"Analyze confusion matrices across all ensemble runs\"\"\"\n",
    "    \n",
    "    # Extract all confusion matrices\n",
    "    cms = [result['metrics']['confusion_matrix'] for result in ensemble_results]\n",
    "    \n",
    "    # Calculate median confusion matrix\n",
    "    median_cm = np.median(cms, axis=0)\n",
    "    \n",
    "    # Calculate confusion matrix statistics\n",
    "    cm_stats = {\n",
    "        'median': median_cm,\n",
    "        'std': np.std(cms, axis=0),\n",
    "        'min': np.min(cms, axis=0),\n",
    "        'max': np.max(cms, axis=0)\n",
    "    }\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot median confusion matrix\n",
    "    sns.heatmap(median_cm, annot=True, fmt='.1f', cmap='Blues', ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Median Confusion Matrix')\n",
    "    axes[0, 0].set_xlabel('Predicted')\n",
    "    axes[0, 0].set_ylabel('True')\n",
    "    axes[0, 0].set_xticklabels(['Non-ischemic', 'Ischemic'])\n",
    "    axes[0, 0].set_yticklabels(['Non-ischemic', 'Ischemic'])\n",
    "    \n",
    "    # Plot standard deviation of confusion matrix\n",
    "    sns.heatmap(cm_stats['std'], annot=True, fmt='.2f', cmap='Reds', ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('Standard Deviation of Confusion Matrix')\n",
    "    axes[0, 1].set_xlabel('Predicted')\n",
    "    axes[0, 1].set_ylabel('True')\n",
    "    axes[0, 1].set_xticklabels(['Non-ischemic', 'Ischemic'])\n",
    "    axes[0, 1].set_yticklabels(['Non-ischemic', 'Ischemic'])\n",
    "    \n",
    "    # Plot confusion matrix from best run (highest accuracy)\n",
    "    best_run = max(ensemble_results, key=lambda x: x['metrics']['accuracy'])\n",
    "    sns.heatmap(best_run['metrics']['confusion_matrix'], annot=True, fmt='d', cmap='Greens', ax=axes[1, 0])\n",
    "    axes[1, 0].set_title(f'Best Run (Accuracy: {best_run[\"metrics\"][\"accuracy\"]:.3f})')\n",
    "    axes[1, 0].set_xlabel('Predicted')\n",
    "    axes[1, 0].set_ylabel('True')\n",
    "    axes[1, 0].set_xticklabels(['Non-ischemic', 'Ischemic'])\n",
    "    axes[1, 0].set_yticklabels(['Non-ischemic', 'Ischemic'])\n",
    "    \n",
    "    # Plot confusion matrix from worst run (lowest accuracy)\n",
    "    worst_run = min(ensemble_results, key=lambda x: x['metrics']['accuracy'])\n",
    "    sns.heatmap(worst_run['metrics']['confusion_matrix'], annot=True, fmt='d', cmap='Oranges', ax=axes[1, 1])\n",
    "    axes[1, 1].set_title(f'Worst Run (Accuracy: {worst_run[\"metrics\"][\"accuracy\"]:.3f})')\n",
    "    axes[1, 1].set_xlabel('Predicted')\n",
    "    axes[1, 1].set_ylabel('True')\n",
    "    axes[1, 1].set_xticklabels(['Non-ischemic', 'Ischemic'])\n",
    "    axes[1, 1].set_yticklabels(['Non-ischemic', 'Ischemic'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate and display key metrics from confusion matrix\n",
    "    median_tn, median_fp, median_fn, median_tp = median_cm.ravel()\n",
    "    \n",
    "    print(\"\\nConfusion Matrix Analysis:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Median True Negatives: {median_tn:.1f}\")\n",
    "    print(f\"Median False Positives: {median_fp:.1f}\")\n",
    "    print(f\"Median False Negatives: {median_fn:.1f}\")\n",
    "    print(f\"Median True Positives: {median_tp:.1f}\")\n",
    "    print()\n",
    "    print(f\"Median False Positive Rate: {median_fp / (median_fp + median_tn):.3f}\")\n",
    "    print(f\"Median False Negative Rate: {median_fn / (median_fn + median_tp):.3f}\")\n",
    "    \n",
    "    # Analyze consistency of predictions\n",
    "    prediction_consistency = []\n",
    "    for img_idx in range(len(true_labels)):\n",
    "        predictions_for_image = [r['predictions'][img_idx] for r in ensemble_results]\n",
    "        consistency = np.mean(predictions_for_image)\n",
    "        prediction_consistency.append({\n",
    "            'image': img_idx + 1,\n",
    "            'true_label': true_labels[img_idx],\n",
    "            'consistency': consistency,\n",
    "            'pred_0_count': predictions_for_image.count(0),\n",
    "            'pred_1_count': predictions_for_image.count(1)\n",
    "        })\n",
    "    \n",
    "    consistency_df = pd.DataFrame(prediction_consistency)\n",
    "    \n",
    "    # Show images with most inconsistent predictions\n",
    "    print(\"\\nMost Inconsistent Predictions:\")\n",
    "    print(\"-\" * 40)\n",
    "    inconsistent = consistency_df[\n",
    "        (consistency_df['consistency'] >= 0.2) & \n",
    "        (consistency_df['consistency'] <= 0.8)\n",
    "    ].sort_values('consistency')\n",
    "    \n",
    "    if len(inconsistent) > 0:\n",
    "        print(inconsistent)\n",
    "    else:\n",
    "        print(\"All predictions are highly consistent across runs.\")\n",
    "    \n",
    "    return cm_stats, consistency_df\n",
    "\n",
    "# Run the confusion matrix analysis\n",
    "cm_stats, consistency_df = analyze_confusion_matrices(ensemble_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Analyze ensemble performance distribution\n",
    "def plot_ensemble_metrics_distribution(ensemble_results):\n",
    "    \"\"\"Plot distribution of metrics across all ensemble runs\"\"\"\n",
    "    metrics_df = pd.DataFrame([{\n",
    "        'run': r['run_number'],\n",
    "        'accuracy': r['metrics']['accuracy'],\n",
    "        'precision': r['metrics']['precision'],\n",
    "        'recall': r['metrics']['recall'],\n",
    "        'specificity': r['metrics']['specificity'],\n",
    "        'f1_score': r['metrics']['f1_score'],\n",
    "        'auc': r['metrics']['auc'] if 'auc' in r['metrics'] else np.nan\n",
    "    } for r in ensemble_results])\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    fig.suptitle('Distribution of Ensemble Metrics Across Runs', fontsize=16)\n",
    "    \n",
    "    metrics = ['accuracy', 'precision', 'recall', 'specificity', 'f1_score', 'auc']\n",
    "    for idx, metric in enumerate(metrics):\n",
    "        ax = axes[idx // 3, idx % 3]\n",
    "        metrics_df[metric].hist(ax=ax, bins=20)\n",
    "        ax.set_title(metric.capitalize())\n",
    "        ax.set_xlabel('Value')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        \n",
    "        # Add median line\n",
    "        median_val = metrics_df[metric].median()\n",
    "        ax.axvline(median_val, color='red', linestyle='--', \n",
    "                  label=f'Median: {median_val:.3f}')\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Box plot of all metrics\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    metrics_long = metrics_df.melt(id_vars=['run'], var_name='metric', value_name='value')\n",
    "    sns.boxplot(data=metrics_long, x='metric', y='value')\n",
    "    plt.title('Distribution of Ensemble Metrics')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_ensemble_metrics_distribution(ensemble_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Ensembles\n",
    "\n",
    "**What this does:**\n",
    "- Combines predictions from multiple models using voting\n",
    "- Creates one ensemble per run (matching models by `tag`)\n",
    "- Calculates metrics for each ensemble\n",
    "\n",
    "**To use:** Set `models_to_use` to the model names from the Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Analyze prediction consistency across runs\n",
    "def analyze_prediction_consistency(ensemble_results):\n",
    "    \"\"\"Analyze how consistent predictions are across different runs\"\"\"\n",
    "    # Stack all predictions\n",
    "    all_predictions = np.vstack([r['predictions'] for r in ensemble_results])\n",
    "    \n",
    "    # Calculate statistics per image\n",
    "    prediction_stats = []\n",
    "    for img_idx in range(len(true_labels)):\n",
    "        img_preds = all_predictions[:, img_idx]\n",
    "        \n",
    "        stats = {\n",
    "            'image_number': img_idx + 1,\n",
    "            'true_label': true_labels[img_idx],\n",
    "            'mean_prediction': np.mean(img_preds),\n",
    "            'std_prediction': np.std(img_preds),\n",
    "            'pred_0_count': np.sum(img_preds == 0),\n",
    "            'pred_1_count': np.sum(img_preds == 1),\n",
    "            'majority_prediction': int(np.mean(img_preds) >= 0.5),\n",
    "            'consistency': np.max([np.sum(img_preds == 0), np.sum(img_preds == 1)]) / len(img_preds)\n",
    "        }\n",
    "        stats['correct'] = stats['majority_prediction'] == stats['true_label']\n",
    "        prediction_stats.append(stats)\n",
    "    \n",
    "    stats_df = pd.DataFrame(prediction_stats)\n",
    "    \n",
    "    # Visualize prediction consistency\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.scatter(stats_df['image_number'], stats_df['consistency'], \n",
    "               c=stats_df['correct'], cmap='RdYlBu', s=100)\n",
    "    plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "    plt.xlabel('Image Number')\n",
    "    plt.ylabel('Prediction Consistency')\n",
    "    plt.title('Prediction Consistency Across Ensemble Runs')\n",
    "    plt.colorbar(label='Correct Prediction')\n",
    "    \n",
    "    # Add annotations for inconsistent predictions\n",
    "    inconsistent = stats_df[stats_df['consistency'] < 0.8]\n",
    "    for _, row in inconsistent.iterrows():\n",
    "        plt.annotate(f\"Img {int(row['image_number'])}\", \n",
    "                    (row['image_number'], row['consistency']),\n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return stats_df\n",
    "\n",
    "prediction_stats = analyze_prediction_consistency(ensemble_results)\n",
    "\n",
    "# Show images with highest uncertainty\n",
    "print(\"\\nImages with highest uncertainty (lowest consistency):\")\n",
    "print(prediction_stats.sort_values('consistency').head(10)[\n",
    "    ['image_number', 'true_label', 'consistency', 'pred_0_count', 'pred_1_count', 'correct']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Create heatmap for a specific run\n",
    "def create_vote_heatmap_for_run(model_names, run_number, voting_method='soft'):\n",
    "    \"\"\"Create a heatmap showing model predictions for a specific run\"\"\"\n",
    "    vote_details = []\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        # Get model data for specific run\n",
    "        model_data = df_models[(df_models['model_name'] == model_name) & \n",
    "                              (df_models['tag'] == run_number)]\n",
    "        \n",
    "        if len(model_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        preds = parse_predictions(model_data.iloc[0]['predicts'])\n",
    "        \n",
    "        if preds is not None:\n",
    "            preds = preds[:46]\n",
    "            vote_details.append({\n",
    "                'model': model_name,\n",
    "                'predictions': preds,\n",
    "                'binary_predictions': np.round(preds)\n",
    "            })\n",
    "    \n",
    "    # Create ensemble prediction for this run\n",
    "    ensemble_pred, ensemble_prob = create_ensemble_by_run(model_names, run_number, voting_method)\n",
    "    \n",
    "    # Create visualization data\n",
    "    vote_data = []\n",
    "    for img_idx in range(len(true_labels)):\n",
    "        img_result = {'image_number': img_idx + 1, 'true_label': true_labels[img_idx]}\n",
    "        for detail in vote_details:\n",
    "            img_result[f\"{detail['model']}_vote\"] = detail['binary_predictions'][img_idx]\n",
    "        img_result['ensemble_prediction'] = ensemble_pred[img_idx]\n",
    "        vote_data.append(img_result)\n",
    "    \n",
    "    vote_df = pd.DataFrame(vote_data)\n",
    "    \n",
    "    # Create heatmap\n",
    "    vote_cols = [f\"{model}_vote\" for model in model_names] + ['ensemble_prediction']\n",
    "    heatmap_data = vote_df[['image_number', 'true_label'] + vote_cols].copy()\n",
    "    \n",
    "    # Create correctness array\n",
    "    correctness_data = np.zeros((len(heatmap_data), len(vote_cols)))\n",
    "    for i, row in heatmap_data.iterrows():\n",
    "        for j, col in enumerate(vote_cols):\n",
    "            correctness_data[i, j] = 1 if row[col] == row['true_label'] else -1\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    cmap = plt.cm.colors.ListedColormap(['red', 'lightblue'])\n",
    "    im = ax.imshow(correctness_data, cmap=cmap, aspect='auto')\n",
    "    \n",
    "    # Set labels\n",
    "    ax.set_xticks(np.arange(len(vote_cols)))\n",
    "    ax.set_yticks(np.arange(len(heatmap_data)))\n",
    "    ax.set_xticklabels([col.replace('_vote', '').replace('_prediction', '') for col in vote_cols], \n",
    "                      rotation=45, ha='right')\n",
    "    ax.set_yticklabels([f\"Image {i+1}\" for i in range(len(heatmap_data))])\n",
    "    \n",
    "    # Add grid\n",
    "    ax.set_xticks(np.arange(len(vote_cols)+1)-.5, minor=True)\n",
    "    ax.set_yticks(np.arange(len(heatmap_data)+1)-.5, minor=True)\n",
    "    ax.grid(which='minor', color='black', linestyle='-', linewidth=2)\n",
    "    ax.tick_params(which='minor', size=0)\n",
    "    \n",
    "    # Add annotations\n",
    "    for i in range(len(heatmap_data)):\n",
    "        for j in range(len(vote_cols)):\n",
    "            pred_value = int(heatmap_data.iloc[i][vote_cols[j]])\n",
    "            ax.text(j, i, pred_value, ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax, ticks=[-1, 1])\n",
    "    cbar.ax.set_yticklabels(['Incorrect', 'Correct'])\n",
    "    \n",
    "    # Set title\n",
    "    title = f'Model Predictions vs True Labels (Run {run_number})\\n'\n",
    "    title += f'Models: {\", \".join(model_names)}\\n(Red=Incorrect, Blue=Correct)'\n",
    "    ax.set_title(title, fontsize=14, pad=20)\n",
    "    ax.set_xlabel('Models', fontsize=12)\n",
    "    ax.set_ylabel('Images', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show heatmap for a specific run\n",
    "run_to_visualize = 1  # Change this to visualize different runs\n",
    "create_vote_heatmap_for_run(models_to_use, run_to_visualize, voting_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Compare ensemble methods (average vs run-by-run)\n",
    "def compare_ensemble_methods(model_names, voting_method='soft'):\n",
    "    \"\"\"Compare the original averaging method vs the run-by-run method\"\"\"\n",
    "    # Original averaging method (as in previous version)\n",
    "    def create_ensemble_average(model_names, voting_method='soft'):\n",
    "        predictions = []\n",
    "        \n",
    "        for model_name in model_names:\n",
    "            # Get all runs for this model\n",
    "            model_data = df_models[df_models['model_name'] == model_name]\n",
    "            \n",
    "            # Parse predictions\n",
    "            model_predictions = []\n",
    "            for idx, row in model_data.iterrows():\n",
    "                preds = parse_predictions(row['predicts'])\n",
    "                if preds is not None:\n",
    "                    preds = preds[:46]\n",
    "                    model_predictions.append(preds)\n",
    "            \n",
    "            if model_predictions:\n",
    "                # Average predictions across runs for this model\n",
    "                avg_model_preds = np.mean(model_predictions, axis=0)\n",
    "                predictions.append(avg_model_preds)\n",
    "        \n",
    "        if not predictions:\n",
    "            raise ValueError(\"No valid predictions found\")\n",
    "        \n",
    "        # Perform ensemble voting\n",
    "        if voting_method == 'hard':\n",
    "            ensemble_pred = hard_voting_ensemble(predictions)\n",
    "            ensemble_prob = None\n",
    "        else:  # soft voting\n",
    "            ensemble_prob = np.mean(predictions, axis=0)\n",
    "            ensemble_pred = np.round(ensemble_prob)\n",
    "        \n",
    "        return ensemble_pred, ensemble_prob\n",
    "    \n",
    "    # Get results from averaging method\n",
    "    avg_ensemble_pred, avg_ensemble_prob = create_ensemble_average(model_names, voting_method)\n",
    "    avg_metrics = calculate_metrics(true_labels, avg_ensemble_pred, avg_ensemble_prob)\n",
    "    \n",
    "    # Get results from run-by-run method (already computed)\n",
    "    run_metrics = {\n",
    "        'accuracy': np.median([r['metrics']['accuracy'] for r in ensemble_results]),\n",
    "        'precision': np.median([r['metrics']['precision'] for r in ensemble_results]),\n",
    "        'recall': np.median([r['metrics']['recall'] for r in ensemble_results]),\n",
    "        'specificity': np.median([r['metrics']['specificity'] for r in ensemble_results]),\n",
    "        'f1_score': np.median([r['metrics']['f1_score'] for r in ensemble_results]),\n",
    "        'auc': np.median([r['metrics']['auc'] for r in ensemble_results if 'auc' in r['metrics']])\n",
    "    }\n",
    "    \n",
    "    # Compare results\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Metric': ['Accuracy', 'Precision', 'Recall', 'Specificity', 'F1-Score', 'AUC'],\n",
    "        'Averaging Method': [avg_metrics['accuracy'], avg_metrics['precision'], \n",
    "                           avg_metrics['recall'], avg_metrics['specificity'], \n",
    "                           avg_metrics['f1_score'], avg_metrics.get('auc', np.nan)],\n",
    "        'Run-by-Run Method': [run_metrics['accuracy'], run_metrics['precision'], \n",
    "                             run_metrics['recall'], run_metrics['specificity'], \n",
    "                             run_metrics['f1_score'], run_metrics['auc']]\n",
    "    })\n",
    "    \n",
    "    print(\"\\nComparison of Ensemble Methods:\")\n",
    "    print(comparison_df.round(4))\n",
    "    \n",
    "    # Visualize comparison\n",
    "    comparison_df.plot(x='Metric', y=['Averaging Method', 'Run-by-Run Method'], \n",
    "                      kind='bar', figsize=(10, 6))\n",
    "    plt.title('Comparison of Ensemble Methods')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "compare_ensemble_methods(models_to_use, voting_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Compare Different Model Combinations and Voting Strategies\n",
    "def weighted_soft_voting_ensemble(predictions_list, weights):\n",
    "    \"\"\"Perform weighted soft voting ensemble\"\"\"\n",
    "    # Ensure weights sum to 1\n",
    "    weights = np.array(weights) / np.sum(weights)\n",
    "    \n",
    "    # Weighted average probabilities\n",
    "    weighted_probs = np.average(predictions_list, axis=0, weights=weights)\n",
    "    \n",
    "    # Convert to binary predictions\n",
    "    return np.round(weighted_probs), weighted_probs\n",
    "\n",
    "def create_ensemble_with_strategy(model_names, run_number, voting_method='soft', weight_type=None):\n",
    "    \"\"\"Create an ensemble with different voting strategies\"\"\"\n",
    "    predictions = []\n",
    "    weights = []\n",
    "    \n",
    "    # Get individual model performance metrics if needed for weighting\n",
    "    if weight_type:\n",
    "        model_metrics = []\n",
    "        for model_name in model_names:\n",
    "            model_data = df_models[df_models['model_name'] == model_name]\n",
    "            if weight_type == 'auc':\n",
    "                # Extract AUC from test metrics - need to calculate from ROC curve\n",
    "                all_aucs = []\n",
    "                for idx, row in model_data.iterrows():\n",
    "                    try:\n",
    "                        fpr = np.array(ast.literal_eval(row['roc_curve_fpr']))\n",
    "                        tpr = np.array(ast.literal_eval(row['roc_curve_tpr']))\n",
    "                        auc = np.trapz(tpr, fpr)\n",
    "                        all_aucs.append(auc)\n",
    "                    except:\n",
    "                        pass\n",
    "                metric_value = np.median(all_aucs) if all_aucs else 0.5\n",
    "            elif weight_type == 'sensitivity':\n",
    "                metric_value = model_data['test_recall'].median()\n",
    "            model_metrics.append(metric_value)\n",
    "        \n",
    "        weights = model_metrics\n",
    "    \n",
    "    # Get predictions for each model\n",
    "    for model_name in model_names:\n",
    "        model_data = df_models[df_models['model_name'] == model_name].reset_index(drop=True)\n",
    "        \n",
    "        if run_number > len(model_data):\n",
    "            continue\n",
    "        \n",
    "        specific_run = model_data.iloc[run_number - 1]\n",
    "        preds = parse_predictions(specific_run['predicts'])\n",
    "        \n",
    "        if preds is not None:\n",
    "            preds = preds[:46]\n",
    "            predictions.append(preds)\n",
    "    \n",
    "    if not predictions:\n",
    "        raise ValueError(f\"No valid predictions found for run {run_number}\")\n",
    "    \n",
    "    # Perform voting based on method\n",
    "    if voting_method == 'hard':\n",
    "        ensemble_pred = hard_voting_ensemble(predictions)\n",
    "        ensemble_prob = None\n",
    "    elif voting_method == 'soft':\n",
    "        ensemble_prob = np.mean(predictions, axis=0)\n",
    "        ensemble_pred = np.round(ensemble_prob)\n",
    "    elif voting_method == 'weighted' and weights:\n",
    "        ensemble_pred, ensemble_prob = weighted_soft_voting_ensemble(predictions, weights)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid voting method: {voting_method}\")\n",
    "    \n",
    "    return ensemble_pred, ensemble_prob\n",
    "\n",
    "# Define model combinations to test\n",
    "# Replace these with the model names from the Excel file\n",
    "# Typically: select top-performing models based on validation accuracy\n",
    "top_3_models = ['Xception', 'InceptionResNetV2', 'DenseNet201']\n",
    "top_5_models = ['VGG19', 'InceptionResNetV2', 'DenseNet201', 'Xception', 'MobileNetV2']\n",
    "\n",
    "# Define voting strategies\n",
    "voting_strategies = [\n",
    "    {'method': 'hard', 'weight_type': None, 'name': 'Hard Voting'},\n",
    "    {'method': 'soft', 'weight_type': None, 'name': 'Soft Voting'},\n",
    "    {'method': 'weighted', 'weight_type': 'auc', 'name': 'Weighted Voting (AUC)'},\n",
    "    {'method': 'weighted', 'weight_type': 'sensitivity', 'name': 'Weighted Voting (Sensitivity)'}\n",
    "]\n",
    "\n",
    "# Run ensemble comparison\n",
    "ensemble_comparison_results = []\n",
    "\n",
    "for model_set_name, model_set in [('Top-3', top_3_models), ('Top-5', top_5_models)]:\n",
    "    for strategy in voting_strategies:\n",
    "        # Run ensemble for all 100 runs\n",
    "        strategy_results = []\n",
    "        \n",
    "        for run_num in range(1, 101):\n",
    "            try:\n",
    "                ensemble_pred, ensemble_prob = create_ensemble_with_strategy(\n",
    "                    model_set, run_num, \n",
    "                    voting_method=strategy['method'],\n",
    "                    weight_type=strategy['weight_type']\n",
    "                )\n",
    "                metrics = calculate_metrics(true_labels, ensemble_pred, ensemble_prob)\n",
    "                strategy_results.append(metrics)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        if strategy_results:\n",
    "            # Calculate median metrics\n",
    "            median_metrics = {\n",
    "                'accuracy': np.median([r['accuracy'] for r in strategy_results]),\n",
    "                'precision': np.median([r['precision'] for r in strategy_results]),\n",
    "                'recall': np.median([r['recall'] for r in strategy_results]),\n",
    "                'specificity': np.median([r['specificity'] for r in strategy_results]),\n",
    "                'f1_score': np.median([r['f1_score'] for r in strategy_results]),\n",
    "                'auc': np.median([r['auc'] for r in strategy_results if 'auc' in r])\n",
    "            }\n",
    "            \n",
    "            ensemble_comparison_results.append({\n",
    "                'model_set': model_set_name,\n",
    "                'strategy': strategy['name'],\n",
    "                'metrics': median_metrics\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Display Comparison Results\n",
    "# Create comparison table\n",
    "comparison_data = []\n",
    "for result in ensemble_comparison_results:\n",
    "    row = {\n",
    "        'Model Set': result['model_set'],\n",
    "        'Voting Strategy': result['strategy'],\n",
    "        'Accuracy': result['metrics']['accuracy'],\n",
    "        'Precision': result['metrics']['precision'],\n",
    "        'Recall': result['metrics']['recall'],\n",
    "        'Specificity': result['metrics']['specificity'],\n",
    "        'F1-Score': result['metrics']['f1_score'],\n",
    "        'AUC': result['metrics']['auc']\n",
    "    }\n",
    "    comparison_data.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nEnsemble Strategy Comparison:\")\n",
    "print(comparison_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10.5: Statistical Testing for Ensemble Comparison\n",
    "from scipy.stats import wilcoxon, ttest_rel\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "\n",
    "def statistical_testing(ensemble_comparison_results, true_labels):\n",
    "    \"\"\"Perform statistical tests to compare the best method against others\"\"\"\n",
    "    \n",
    "    # First, collect all predictions and metrics for each configuration\n",
    "    config_results = {}\n",
    "    \n",
    "    # Rerun to collect all individual predictions\n",
    "    for model_set_name, model_set in [('Top-3', top_3_models), ('Top-5', top_5_models)]:\n",
    "        for strategy in voting_strategies:\n",
    "            config_name = f\"{model_set_name} - {strategy['name']}\"\n",
    "            config_results[config_name] = {\n",
    "                'predictions': [],\n",
    "                'accuracies': [],\n",
    "                'f1_scores': [],\n",
    "                'aucs': []\n",
    "            }\n",
    "            \n",
    "            # Collect results for all 100 runs\n",
    "            for run_num in range(1, 101):\n",
    "                try:\n",
    "                    ensemble_pred, ensemble_prob = create_ensemble_with_strategy(\n",
    "                        model_set, run_num, \n",
    "                        voting_method=strategy['method'],\n",
    "                        weight_type=strategy['weight_type']\n",
    "                    )\n",
    "                    metrics = calculate_metrics(true_labels, ensemble_pred, ensemble_prob)\n",
    "                    \n",
    "                    config_results[config_name]['predictions'].append(ensemble_pred)\n",
    "                    config_results[config_name]['accuracies'].append(metrics['accuracy'])\n",
    "                    config_results[config_name]['f1_scores'].append(metrics['f1_score'])\n",
    "                    config_results[config_name]['aucs'].append(metrics['auc'])\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "    \n",
    "    # Identify the best configuration (Top-3 with Hard Voting)\n",
    "    best_config = \"Top-3 - Hard Voting\"\n",
    "    best_results = config_results[best_config]\n",
    "    \n",
    "    # Prepare results table\n",
    "    statistical_results = []\n",
    "    \n",
    "    for config_name, results in config_results.items():\n",
    "        if config_name == best_config:\n",
    "            continue\n",
    "        \n",
    "        # Check if we have enough results for statistical testing\n",
    "        if len(results['accuracies']) == 0 or len(best_results['accuracies']) == 0:\n",
    "            print(f\"Skipping {config_name} - insufficient data for comparison\")\n",
    "            statistical_results.append({\n",
    "                'Comparison': f\"{best_config} vs {config_name}\",\n",
    "                'Wilcoxon_Accuracy_p': np.nan,\n",
    "                'Wilcoxon_F1_p': np.nan,\n",
    "                'Wilcoxon_AUC_p': np.nan,\n",
    "                'T-test_Accuracy_p': np.nan,\n",
    "                'T-test_F1_p': np.nan,\n",
    "                'T-test_AUC_p': np.nan,\n",
    "                'McNemar_p': np.nan\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Initialize p-values\n",
    "        p_values = {\n",
    "            'accuracy': np.nan,\n",
    "            'f1': np.nan,\n",
    "            'auc': np.nan,\n",
    "            'accuracy_t': np.nan,\n",
    "            'f1_t': np.nan,\n",
    "            'auc_t': np.nan,\n",
    "            'mcnemar': np.nan\n",
    "        }\n",
    "        \n",
    "        # Wilcoxon signed-rank test for paired continuous metrics\n",
    "        if len(best_results['accuracies']) > 0 and len(results['accuracies']) > 0:\n",
    "            try:\n",
    "                accuracy_test = wilcoxon(best_results['accuracies'], results['accuracies'])\n",
    "                p_values['accuracy'] = accuracy_test.pvalue\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if len(best_results['f1_scores']) > 0 and len(results['f1_scores']) > 0:\n",
    "            try:\n",
    "                f1_test = wilcoxon(best_results['f1_scores'], results['f1_scores'])\n",
    "                p_values['f1'] = f1_test.pvalue\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if len(best_results['aucs']) > 0 and len(results['aucs']) > 0:\n",
    "            try:\n",
    "                auc_test = wilcoxon(best_results['aucs'], results['aucs'])\n",
    "                p_values['auc'] = auc_test.pvalue\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Paired t-test as alternative\n",
    "        if len(best_results['accuracies']) > 0 and len(results['accuracies']) > 0:\n",
    "            try:\n",
    "                accuracy_ttest = ttest_rel(best_results['accuracies'], results['accuracies'])\n",
    "                p_values['accuracy_t'] = accuracy_ttest.pvalue\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if len(best_results['f1_scores']) > 0 and len(results['f1_scores']) > 0:\n",
    "            try:\n",
    "                f1_ttest = ttest_rel(best_results['f1_scores'], results['f1_scores'])\n",
    "                p_values['f1_t'] = f1_ttest.pvalue\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if len(best_results['aucs']) > 0 and len(results['aucs']) > 0:\n",
    "            try:\n",
    "                auc_ttest = ttest_rel(best_results['aucs'], results['aucs'])\n",
    "                p_values['auc_t'] = auc_ttest.pvalue\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # McNemar's test for paired binary predictions (using first run as example)\n",
    "        if len(best_results['predictions']) > 0 and len(results['predictions']) > 0:\n",
    "            try:\n",
    "                contingency_table = np.zeros((2, 2))\n",
    "                best_pred = best_results['predictions'][0]\n",
    "                other_pred = results['predictions'][0]\n",
    "                \n",
    "                # Fill contingency table\n",
    "                for i in range(len(true_labels)):\n",
    "                    if best_pred[i] == true_labels[i] and other_pred[i] == true_labels[i]:\n",
    "                        contingency_table[0, 0] += 1  # Both correct\n",
    "                    elif best_pred[i] == true_labels[i] and other_pred[i] != true_labels[i]:\n",
    "                        contingency_table[0, 1] += 1  # Only best correct\n",
    "                    elif best_pred[i] != true_labels[i] and other_pred[i] == true_labels[i]:\n",
    "                        contingency_table[1, 0] += 1  # Only other correct\n",
    "                    else:\n",
    "                        contingency_table[1, 1] += 1  # Both wrong\n",
    "                \n",
    "                mcnemar_result = mcnemar(contingency_table, exact=True)\n",
    "                p_values['mcnemar'] = mcnemar_result.pvalue\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        statistical_results.append({\n",
    "            'Comparison': f\"{best_config} vs {config_name}\",\n",
    "            'Wilcoxon_Accuracy_p': p_values['accuracy'],\n",
    "            'Wilcoxon_F1_p': p_values['f1'],\n",
    "            'Wilcoxon_AUC_p': p_values['auc'],\n",
    "            'T-test_Accuracy_p': p_values['accuracy_t'],\n",
    "            'T-test_F1_p': p_values['f1_t'],\n",
    "            'T-test_AUC_p': p_values['auc_t'],\n",
    "            'McNemar_p': p_values['mcnemar']\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    stat_df = pd.DataFrame(statistical_results)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nStatistical Testing Results - Comparing {best_config} against other methods:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Format p-values for better readability\n",
    "    pd.set_option('display.float_format', lambda x: '%.6f' % x)\n",
    "    print(stat_df)\n",
    "    \n",
    "    # Interpretation\n",
    "    print(\"\\nInterpretation (p < 0.05 indicates significant difference):\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    significant_comparisons = []\n",
    "    for idx, row in stat_df.iterrows():\n",
    "        if pd.notna(row['Wilcoxon_Accuracy_p']) and row['Wilcoxon_Accuracy_p'] < 0.05:\n",
    "            significant_comparisons.append(f\"{row['Comparison']} - Accuracy (p={row['Wilcoxon_Accuracy_p']:.6f})\")\n",
    "        if pd.notna(row['Wilcoxon_F1_p']) and row['Wilcoxon_F1_p'] < 0.05:\n",
    "            significant_comparisons.append(f\"{row['Comparison']} - F1-Score (p={row['Wilcoxon_F1_p']:.6f})\")\n",
    "        if pd.notna(row['Wilcoxon_AUC_p']) and row['Wilcoxon_AUC_p'] < 0.05:\n",
    "            significant_comparisons.append(f\"{row['Comparison']} - AUC (p={row['Wilcoxon_AUC_p']:.6f})\")\n",
    "    \n",
    "    if significant_comparisons:\n",
    "        print(f\"{best_config} is significantly better than other methods in:\")\n",
    "        for comp in significant_comparisons:\n",
    "            print(f\"  - {comp}\")\n",
    "    else:\n",
    "        print(\"No statistically significant differences found.\")\n",
    "    \n",
    "    # Create visualization of p-values\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Prepare data for heatmap\n",
    "    metrics = ['Accuracy', 'F1-Score', 'AUC']\n",
    "    comparisons = stat_df['Comparison'].tolist()\n",
    "    \n",
    "    p_values = np.zeros((len(comparisons), len(metrics)))\n",
    "    for i, metric in enumerate(metrics):\n",
    "        p_values[:, i] = stat_df[f'Wilcoxon_{metric.replace(\"-Score\", \"\")}_p'].fillna(1.0).values\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.imshow(p_values, cmap='RdYlGn_r', aspect='auto')\n",
    "    plt.colorbar(label='p-value')\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(comparisons)):\n",
    "        for j in range(len(metrics)):\n",
    "            if pd.notna(stat_df.iloc[i][f'Wilcoxon_{metrics[j].replace(\"-Score\", \"\")}_p']):\n",
    "                plt.text(j, i, f'{p_values[i, j]:.4f}', \n",
    "                        ha='center', va='center', \n",
    "                        color='white' if p_values[i, j] > 0.5 else 'black')\n",
    "            else:\n",
    "                plt.text(j, i, 'N/A', \n",
    "                        ha='center', va='center', \n",
    "                        color='black')\n",
    "    \n",
    "    plt.xticks(range(len(metrics)), metrics)\n",
    "    plt.yticks(range(len(comparisons)), [c.replace(f\"{best_config} vs \", \"\") for c in comparisons])\n",
    "    plt.title(f'Statistical Significance (Wilcoxon test) - {best_config} vs Others')\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Comparison')\n",
    "    \n",
    "    # Add significance threshold line\n",
    "    plt.axhline(y=-0.5, color='red', linestyle='--', linewidth=2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return stat_df\n",
    "\n",
    "# Run statistical testing\n",
    "stat_results = statistical_testing(ensemble_comparison_results, true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Visualize Comparison Results\n",
    "def plot_ensemble_comparison(comparison_df):\n",
    "    \"\"\"Create visualizations for ensemble comparison\"\"\"\n",
    "    \n",
    "    # Reshape data for plotting\n",
    "    metrics_for_plot = comparison_df.melt(\n",
    "        id_vars=['Model Set', 'Voting Strategy'],\n",
    "        var_name='Metric',\n",
    "        value_name='Score'\n",
    "    )\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Ensemble Comparison: Top-3 vs Top-5 with Different Voting Strategies', fontsize=16)\n",
    "    \n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'Specificity', 'F1-Score', 'AUC']\n",
    "    \n",
    "    for idx, metric in enumerate(metrics):\n",
    "        ax = axes[idx // 3, idx % 3]\n",
    "        \n",
    "        metric_data = metrics_for_plot[metrics_for_plot['Metric'] == metric]\n",
    "        \n",
    "        # Create grouped bar plot\n",
    "        sns.barplot(data=metric_data, x='Model Set', y='Score', hue='Voting Strategy', ax=ax)\n",
    "        \n",
    "        ax.set_title(metric)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('Score')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for container in ax.containers:\n",
    "            ax.bar_label(container, fmt='%.3f', padding=3)\n",
    "        \n",
    "        # Adjust legend\n",
    "        if idx == 0:\n",
    "            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        else:\n",
    "            ax.get_legend().remove()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create separate plot for best performers\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    best_by_metric = {}\n",
    "    for metric in metrics:\n",
    "        best_idx = comparison_df[metric].idxmax()\n",
    "        best_row = comparison_df.iloc[best_idx]\n",
    "        best_by_metric[metric] = f\"{best_row['Model Set']} - {best_row['Voting Strategy']}\"\n",
    "    \n",
    "    best_performers = pd.Series(best_by_metric)\n",
    "    best_performers.value_counts().plot(kind='bar')\n",
    "    plt.title('Best Performing Configuration by Metric')\n",
    "    plt.xlabel('Ensemble Configuration')\n",
    "    plt.ylabel('Number of Metrics Where Best')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_ensemble_comparison(comparison_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Statistical Comparison\n",
    "def statistical_analysis(comparison_df):\n",
    "    \"\"\"Perform statistical analysis of ensemble strategies\"\"\"\n",
    "    \n",
    "    # Find best configuration for each metric\n",
    "    print(\"\\nBest Configuration for Each Metric:\")\n",
    "    print(\"-\" * 50)\n",
    "    for metric in ['Accuracy', 'Precision', 'Recall', 'Specificity', 'F1-Score', 'AUC']:\n",
    "        best_idx = comparison_df[metric].idxmax()\n",
    "        best_row = comparison_df.iloc[best_idx]\n",
    "        print(f\"{metric}: {best_row['Model Set']} with {best_row['Voting Strategy']} = {best_row[metric]:.4f}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nSummary Statistics by Model Set:\")\n",
    "    print(\"-\" * 50)\n",
    "    summary = comparison_df.groupby('Model Set').agg({\n",
    "        'Accuracy': ['mean', 'std', 'max', 'min'],\n",
    "        'AUC': ['mean', 'std', 'max', 'min']\n",
    "    }).round(4)\n",
    "    print(summary)\n",
    "    \n",
    "    print(\"\\nSummary Statistics by Voting Strategy:\")\n",
    "    print(\"-\" * 50)\n",
    "    summary = comparison_df.groupby('Voting Strategy').agg({\n",
    "        'Accuracy': ['mean', 'std', 'max', 'min'],\n",
    "        'AUC': ['mean', 'std', 'max', 'min']\n",
    "    }).round(4)\n",
    "    print(summary)\n",
    "    \n",
    "    # Performance gain analysis\n",
    "    print(\"\\nPerformance Gain Analysis:\")\n",
    "    print(\"-\" * 50)\n",
    "    baseline = comparison_df[(comparison_df['Model Set'] == 'Top-3') & \n",
    "                           (comparison_df['Voting Strategy'] == 'Hard Voting')].iloc[0]\n",
    "    \n",
    "    for idx, row in comparison_df.iterrows():\n",
    "        if idx == 0:  # Skip baseline\n",
    "            continue\n",
    "        gain = {\n",
    "            'Configuration': f\"{row['Model Set']} - {row['Voting Strategy']}\",\n",
    "            'Accuracy Gain': row['Accuracy'] - baseline['Accuracy'],\n",
    "            'AUC Gain': row['AUC'] - baseline['AUC'],\n",
    "            'F1 Gain': row['F1-Score'] - baseline['F1-Score']\n",
    "        }\n",
    "        print(f\"{gain['Configuration']}:\")\n",
    "        print(f\"  Accuracy: {gain['Accuracy Gain']:+.4f}\")\n",
    "        print(f\"  AUC: {gain['AUC Gain']:+.4f}\")\n",
    "        print(f\"  F1-Score: {gain['F1 Gain']:+.4f}\")\n",
    "        print()\n",
    "\n",
    "statistical_analysis(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4.9 \u2014 Run-by-run metrics & IQR check (ensemble + optional base models)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 1) Build run-by-run metrics table from your existing `ensemble_results` ---\n",
    "\n",
    "def ensemble_runs_to_dataframe(ensemble_results):\n",
    "    \"\"\"Convert `ensemble_results` (list of dicts) to a tidy per-run metrics DataFrame.\"\"\"\n",
    "    rows = []\n",
    "    for r in ensemble_results:\n",
    "        m = r['metrics']\n",
    "        rows.append({\n",
    "            'Run': r['run_number'],\n",
    "            'ACC': m.get('accuracy', np.nan),\n",
    "            'PRE': m.get('precision', np.nan),\n",
    "            'SEN': m.get('recall', np.nan),\n",
    "            'SPE': m.get('specificity', np.nan),\n",
    "            'F1S': m.get('f1_score', np.nan),\n",
    "            'AUC': m.get('auc', np.nan)  # will be NaN for hard voting\n",
    "        })\n",
    "    df = pd.DataFrame(rows).sort_values('Run', kind='stable').reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def summarize_variation(df_runs):\n",
    "    \"\"\"Return Median, IQR, and n_unique per metric across runs + determinism check on predictions.\"\"\"\n",
    "    metrics = ['ACC','PRE','SEN','SPE','F1S','AUC']\n",
    "    present = [m for m in metrics if m in df_runs.columns]\n",
    "    X = df_runs[present]\n",
    "\n",
    "    med = X.median()\n",
    "    q1  = X.quantile(0.25)\n",
    "    q3  = X.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    nunique = X.nunique()\n",
    "\n",
    "    summary = (\n",
    "        pd.DataFrame({'Median': med, 'IQR': iqr, 'n_unique': nunique})\n",
    "        .round(6)\n",
    "        .reset_index()\n",
    "        .rename(columns={'index':'Metric'})\n",
    "    )\n",
    "\n",
    "    # All metrics identical across runs?\n",
    "    all_metrics_identical = nunique.eq(1).all()\n",
    "\n",
    "    return summary, bool(all_metrics_identical)\n",
    "\n",
    "def count_unique_prediction_vectors(ensemble_results):\n",
    "    \"\"\"\n",
    "    Check how many distinct 0/1 prediction vectors the ensemble produced across runs.\n",
    "    If =1, predictions are identical every run (deterministic).\n",
    "    \"\"\"\n",
    "    preds = np.vstack([r['predictions'] for r in ensemble_results])  # shape: (R, 46)\n",
    "    unique_pred_rows = np.unique(preds, axis=0).shape[0]\n",
    "    return int(unique_pred_rows)\n",
    "\n",
    "# Build per-run table\n",
    "ens_runs_df = ensemble_runs_to_dataframe(ensemble_results)\n",
    "\n",
    "# Summarize Median / IQR / n_unique\n",
    "summary_df, all_same = summarize_variation(ens_runs_df)\n",
    "\n",
    "# Determinism check based on full prediction vectors\n",
    "n_unique_pred_vectors = count_unique_prediction_vectors(ensemble_results)\n",
    "\n",
    "# Display\n",
    "display(\n",
    "    ens_runs_df.style\n",
    "    .set_caption(f\"Run-by-Run Metrics \u2014 Ensemble ({voting_method}): \" + \", \".join(models_to_use))\n",
    "    .set_properties(**{'text-align': 'center'})\n",
    ")\n",
    "print(\"\\nSummary across runs (Median / IQR / n_unique):\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nAll metrics identical across runs? -> {all_same}\")\n",
    "print(f\"Number of unique prediction vectors across runs -> {n_unique_pred_vectors}\")\n",
    "\n",
    "# Optional: save to CSV\n",
    "ens_runs_df.to_csv(\"ensemble_run_by_run_metrics.csv\", index=False)\n",
    "summary_df.to_csv(\"ensemble_run_by_run_summary.csv\", index=False)\n",
    "print(\"\\nSaved CSVs: ensemble_run_by_run_metrics.csv, ensemble_run_by_run_summary.csv\")\n",
    "\n",
    "# --- 2) (Optional) Per-base-model run-by-run metrics from df_models -----------------\n",
    "\n",
    "def model_run_by_run_metrics(df_models, model_name, true_labels, threshold=0.5, sort_by='tag'):\n",
    "    \"\"\"\n",
    "    Compute run-by-run metrics for a single base model using your stored predictions in df_models.\n",
    "    Uses your helpers: parse_predictions + calculate_metrics.\n",
    "    \"\"\"\n",
    "    sub = df_models[df_models['model_name'] == model_name].copy()\n",
    "    if sub.empty:\n",
    "        raise ValueError(f\"No rows for model_name='{model_name}'. \"\n",
    "                         f\"Available: {sorted(df_models['model_name'].unique().tolist())}\")\n",
    "    if sort_by in sub.columns:\n",
    "        sub = sub.sort_values(sort_by, kind='stable')\n",
    "\n",
    "    rows = []\n",
    "    for i, (_, row) in enumerate(sub.iterrows(), start=1):\n",
    "        probs = parse_predictions(row['predicts'])\n",
    "        if probs is None:\n",
    "            continue\n",
    "        probs = np.asarray(probs[:len(true_labels)], dtype=float)\n",
    "        y_pred = (probs >= threshold).astype(int)  # hard 0.5 threshold\n",
    "\n",
    "        m = calculate_metrics(true_labels, y_pred, y_prob=probs)\n",
    "\n",
    "        rows.append({\n",
    "            'Run': i if 'tag' not in sub.columns else row.get('tag', i),\n",
    "            'ACC': m.get('accuracy', np.nan),\n",
    "            'PRE': m.get('precision', np.nan),\n",
    "            'SEN': m.get('recall', np.nan),\n",
    "            'SPE': m.get('specificity', np.nan),\n",
    "            'F1S': m.get('f1_score', np.nan),\n",
    "            'AUC': m.get('auc', np.nan)\n",
    "        })\n",
    "\n",
    "    out = pd.DataFrame(rows).sort_values('Run', kind='stable').reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "# Example usage (uncomment to inspect a base model):\n",
    "# base_df = model_run_by_run_metrics(df_models, 'InceptionResNetV2', true_labels)\n",
    "# display(base_df.style.set_caption(\"Run-by-Run Metrics \u2014 InceptionResNetV2\"))\n",
    "# base_summary, base_all_same = summarize_variation(base_df)\n",
    "# print(\"\\nBase model summary:\")\n",
    "# print(base_summary.to_string(index=False))\n",
    "# print(\"\\nAll metrics identical across runs? ->\", base_all_same)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TL2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}